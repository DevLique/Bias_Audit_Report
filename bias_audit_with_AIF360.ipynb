
# Bias Audit Report Notebook (IBM AIF360 + Jupyter)

This notebook lets you:
- Paste **single pieces of text** to flag potential bias indicators (identity terms, slurs, gendered language, sentiment tilt).
- Audit a **dataset** with a protected attribute column using **IBM AI Fairness 360 (AIF360)**.
- Train a quick baseline classifier and compute fairness metrics like **Statistical Parity Difference**, **Disparate Impact**, **Equal Opportunity Difference**, and **Average Odds Difference**.
- Try a simple **mitigation** (Reweighing) and compare metrics.

> **Expected dataset format (CSV)**:  
> - `text` *(string)* — the content to analyze  
> - `protected_attribute` *(string or int)* — e.g., `female` vs `male`, or `0` vs `1`  
> - `label` *(optional, 0/1 or string convertible to 0/1)* — the ground-truth outcome if you're auditing a predictive task  
>
> You can run the *single-text audit* without any dataset. For fairness metrics you’ll need `label` and `protected_attribute`.


# If running on a fresh environment, install dependencies (uncomment if needed).
# You can skip if you already have these installed.
# Note: Some environments require restart after installs.

# !pip install --quiet aif360==0.6.1 scikit-learn pandas numpy matplotlib nltk imbalanced-learn


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import LabelEncoder

from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing

import re
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except LookupError:
    import nltk
    nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

pd.options.display.max_colwidth = 200

IDENTITY_TERMS = {
    "gender": [
        "woman","women","female","man","men","male","girl","boy","guys","ladies",
        "trans","transgender","nonbinary","genderqueer","she","her","hers","he","him","his","they","them","theirs"
    ],
    "race_ethnicity": [
        "black","white","asian","latino","latina","hispanic","indian","middle eastern","arab",
        "african","european","native","indigenous"
    ],
    "religion": [
        "christian","muslim","islamic","jewish","jew","hindu","buddhist","sikh","atheist"
    ],
    "orientation": [
        "gay","lesbian","bisexual","queer","lgbt","lgbtq","straight","heterosexual"
    ],
    "age": [
        "old","elderly","youth","young","teen","teenage","senior","middle-aged"
    ]
}

# Example slur/offensive lexicon (very minimal & non-exhaustive; augment for production)
SLURS = [
    # This list is intentionally minimal. For production, use vetted, community-reviewed resources.
    "idiot","stupid","dumb","retarded","moron"
]

GENDERED_PHRASES = [
    # Stereotype-y phrases (illustrative only)
    "man up","like a girl","boys will be boys","hysterical woman"
]

def contains_any(text, terms):
    t = text.lower()
    hits = []
    for w in terms:
        if re.search(r'\b' + re.escape(w) + r'\b', t):
            hits.append(w)
    return hits

def text_bias_flags(text: str):
    flags = {}

    # Identity mentions
    id_hits = {}
    for k, vocab in IDENTITY_TERMS.items():
        hits = contains_any(text, vocab)
        if hits:
            id_hits[k] = hits
    if id_hits:
        flags['identity_mentions'] = id_hits

    # Slurs/offensive
    slur_hits = contains_any(text, SLURS)
    if slur_hits:
        flags['offensive_terms'] = slur_hits

    # Gendered phrases
    gp_hits = contains_any(text, GENDERED_PHRASES)
    if gp_hits:
        flags['gendered_phrases'] = gp_hits

    # Sentiment
    s = sia.polarity_scores(text)
    flags['sentiment'] = s

    # Polarity tilt (simple heuristic): very negative with identity mention
    if 'identity_mentions' in flags and s['compound'] <= -0.5:
        flags['note'] = "Strong negative sentiment co-occurs with identity mentions. Review for potential bias."

    return flags

## 🔎 Single-Text Bias Scan
Paste text into `sample_text` and run the cell to see flags.


sample_text = """That candidate is too emotional; he should just man up."""

flags = text_bias_flags(sample_text)
flags


## 📁 Load a Dataset
Set `csv_path` to your file. Required columns:
- `text`
- `protected_attribute`
- `label` *(optional for fairness metrics; needed if training a model)*


# Example:
csv_path = "my_dataset.csv"  

df = pd.read_csv(csv_path)
print("Rows:", len(df))
df.head()


## 🧭 Dataset Overview & Representation
This section inspects representation per group and runs a light text scan per group.


assert 'text' in df.columns and 'protected_attribute' in df.columns, "Dataset must have 'text' and 'protected_attribute' columns."

# Normalize protected attribute to string
df['protected_attribute'] = df['protected_attribute'].astype(str)


has_label = 'label' in df.columns
if has_label:
    # Auto-encode to 0/1 if needed
    if df['label'].dtype != np.number:
        le = LabelEncoder()
        df['label'] = le.fit_transform(df['label'].astype(str))

group_counts = df['protected_attribute'].value_counts(dropna=False)
group_prop = (group_counts / len(df)).rename("proportion")
rep = pd.concat([group_counts.rename("count"), group_prop], axis=1)
rep


def quick_group_scan(df, max_per_group=200):
    rows = []
    for g, sub in df.groupby('protected_attribute'):
        sub_s = sub.sample(min(len(sub), max_per_group), random_state=42)
        identity_hits = 0
        offensive_hits = 0
        gendered_hits = 0
        neg_with_identity = 0

        for txt in sub_s['text'].astype(str).tolist():
            f = text_bias_flags(txt)
            identity_hits += int('identity_mentions' in f)
            offensive_hits += int('offensive_terms' in f)
            gendered_hits += int('gendered_phrases' in f)
            if 'identity_mentions' in f and f['sentiment']['compound'] <= -0.5:
                neg_with_identity += 1

        n = len(sub_s)
        rows.append({
            'group': g,
            'n_scanned': n,
            'pct_identity_mentions': identity_hits / n if n else np.nan,
            'pct_offensive_terms': offensive_hits / n if n else np.nan,
            'pct_gendered_phrases': gendered_hits / n if n else np.nan,
            'pct_neg_sentiment_with_identity': neg_with_identity / n if n else np.nan
        })
    return pd.DataFrame(rows).sort_values('group')

scan_summary = quick_group_scan(df)
scan_summary


## 🤖 Train a Simple Classifier (if `label` exists) & Compute Fairness Metrics
We train a TF‑IDF + Logistic Regression baseline and evaluate fairness with AIF360.

if has_label:
    X_train, X_test, y_train, y_test, g_train, g_test = train_test_split(
        df['text'].astype(str), df['label'].astype(int), df['protected_attribute'].astype(str),
        test_size=0.3, random_state=42, stratify=df['label']
    )

    vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
    Xtr = vect.fit_transform(X_train)
    Xte = vect.transform(X_test)

    clf = LogisticRegression(max_iter=200, n_jobs=None)
    clf.fit(Xtr, y_train)
    proba = clf.predict_proba(Xte)[:,1]
    pred = (proba >= 0.5).astype(int)

    print("Model report (test):")
    model_report = classification_report(y_test, pred, digits=3)
    print(model_report)
    
    try:
        roc_score = roc_auc_score(y_test, proba)
        print("ROC AUC:", roc_score)
    except Exception as e:
        print("ROC AUC unavailable:", e)
        roc_score = None

    # Prepare AIF360 dataset objects
    # Choose an unprivileged and privileged group from the most common values in g_test
    group_vals = pd.Series(g_test).value_counts().index.tolist()
    if len(group_vals) < 2:
        raise ValueError("Need at least two groups in 'protected_attribute' to compute fairness metrics.")
    unpriv = group_vals[-1]
    priv = group_vals[0]

    # Binarize y_test for AIF360 (e.g., map label==0 to 0, label>0 to 1)
    y_test_bin = (y_test > 0).astype(int)

    # Use binarized labels for AIF360
    test_df_for_aif = pd.DataFrame({
        'label': y_test_bin.values,
        'protected_attribute': g_test.values,
        'y_pred': pred
    })

    # Map groups to binary protected attribute for AIF360
    pa_bin = (test_df_for_aif['protected_attribute'] == priv).astype(int)
    
    dataset_true = BinaryLabelDataset(
        favorable_label=1,
        unfavorable_label=0,
        df=pd.DataFrame({'label': test_df_for_aif['label'].values, 'pa': pa_bin.values}),
        label_names=['label'],
        protected_attribute_names=['pa']
    )

    dataset_pred = BinaryLabelDataset(
        favorable_label=1,
        unfavorable_label=0,
        df=pd.DataFrame({'label': test_df_for_aif['y_pred'].values, 'pa': pa_bin.values}),
        label_names=['label'],
        protected_attribute_names=['pa']
    )

    bld_metric = BinaryLabelDatasetMetric(
        dataset_true,
        privileged_groups=[{'pa': 1}],
        unprivileged_groups=[{'pa': 0}]
    )

    clf_metric = ClassificationMetric(
        dataset_true,
        dataset_pred,
        privileged_groups=[{'pa': 1}],
        unprivileged_groups=[{'pa': 0}]
    )

    spd = bld_metric.statistical_parity_difference()
    di = bld_metric.disparate_impact()
    eod = clf_metric.equal_opportunity_difference()
    aod = clf_metric.average_odds_difference()

    baseline_results = {
        "unprivileged_group": unpriv,
        "privileged_group": priv,
        "statistical_parity_difference": spd,
        "disparate_impact": di,
        "equal_opportunity_difference": eod,
        "average_odds_difference": aod
    }
    
    print(baseline_results)

    # Plot metrics (one plot only; default colors)
    metrics = ["Stat Parity Diff (ideal 0)", "Disparate Impact (ideal 1)", "Equal Opp Diff (ideal 0)", "Avg Odds Diff (ideal 0)"]
    values = [spd, di, eod, aod]

    plt.figure()
    plt.bar(metrics, values)
    plt.xticks(rotation=20, ha='right')
    plt.title("Fairness Metrics (Baseline Model)")
    plt.tight_layout()
    
    # CREATE OUTPUTS FOLDER AND SAVE FILES
    import os  # Import os here
    output_dir = os.path.join(os.getcwd(), "bias_audit_outputs")
    os.makedirs(output_dir, exist_ok=True)
    
    # Save the baseline chart
    plt.savefig(os.path.join(output_dir, "baseline_fairness_metrics.png"), dpi=300, bbox_inches='tight')
    plt.show()
    
    # Save model report and metrics as text
    report_text = f"""=== Baseline Model Results ===

Model Performance Report:
{model_report}

ROC AUC: {roc_score if roc_score else 'Unavailable'}

Fairness Metrics:
Unprivileged Group: {unpriv}
Privileged Group: {priv}
Statistical Parity Difference: {spd}
Disparate Impact: {di}
Equal Opportunity Difference: {eod}
Average Odds Difference: {aod}

Notes:
- Values near 0 for SPD/EOD/AOD and near 1 for DI indicate better parity.
"""
    
    with open(os.path.join(output_dir, "baseline_model_results.txt"), 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(f"\n✓ Files saved to: {output_dir}")
    
else:
    print("No 'label' column detected. Skipping model + fairness metrics.")

## 🛠️ Mitigation: Reweighing (AIF360)
We apply **Reweighing** to the training data and retrain the model, then recompute metrics.

if has_label:
    
    # Plot comparison
    metrics = ["Stat Parity Diff", "Disparate Impact", "Equal Opp Diff", "Avg Odds Diff"]
    base_vals = [spd, di, eod, aod]
    rw_vals = [spd_rw, di_rw, eod_rw, aod_rw]

    # One plot only per instruction; show side-by-side bars by offsetting x positions
    x = np.arange(len(metrics))
    width = 0.35

    plt.figure()
    plt.bar(x - width/2, base_vals, width, label='Baseline')
    plt.bar(x + width/2, rw_vals, width, label='Reweighing')
    plt.xticks(x, metrics, rotation=20, ha='right')
    plt.title("Fairness Metrics: Baseline vs Reweighing")
    plt.legend()
    plt.tight_layout()
    
    # CREATE OUTPUTS FOLDER AND SAVE CHART
    import os  # Import os here
    output_dir = os.path.join(os.getcwd(), "bias_audit_outputs")
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(os.path.join(output_dir, "fairness_metrics_comparison.png"), dpi=300, bbox_inches='tight')
    
    plt.show()
    
    print(f"\n✓ Comparison chart saved to: {os.path.join(output_dir, 'fairness_metrics_comparison.png')}")
    
else:
    print("No 'label' column detected. Skipping mitigation.")

## 🧾 Report Summary Helper
This cell prints a concise, copy‑pastable report from the computed artifacts.

import os  

def summarize_report(group_representation=None, scan_summary=None, baseline_metrics=None, mitigated_metrics=None):
    report_content = []
    report_content.append("=== Bias Audit Report ===")
    
    if group_representation is not None:
        report_content.append("\n[Representation by protected group]\n")
        report_content.append(group_representation.to_string())
    
    if scan_summary is not None:
        report_content.append("\n[Group-wise Text Scan]\n")
        report_content.append(scan_summary.to_string(index=False))
    
    if baseline_metrics is not None:
        report_content.append("\n[Baseline Fairness Metrics]\n")
        for k,v in baseline_metrics.items():
            report_content.append(f"{k}: {v}")
    
    if mitigated_metrics is not None:
        report_content.append("\n[Mitigated Fairness Metrics]\n")
        for k,v in mitigated_metrics.items():
            report_content.append(f"{k}: {v}")
    
    report_content.append("\nNotes:")
    report_content.append("- Values near 0 for SPD/EOD/AOD and near 1 for DI indicate better parity.")
    report_content.append("- Review single-text flags for context & language cues not captured by model-level metrics.")
    
    # Print to console
    full_report = "\n".join(report_content)
    print(full_report)
    
    # CREATE OUTPUTS FOLDER AND SAVE REPORT
    try:
        output_dir = os.path.join(os.getcwd(), "bias_audit_outputs")
        os.makedirs(output_dir, exist_ok=True)
        
        file_path = os.path.join(output_dir, "bias_audit_report.txt")
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(full_report)
        
        print(f"\n✓ Report saved to: {file_path}")
        
    except Exception as e:
        print(f"\n❌ Error saving file: {e}")
    
    return full_report

